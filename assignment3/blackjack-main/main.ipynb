{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b521b48",
   "metadata": {},
   "source": [
    "### Question 8 (3 Extra Points):\n",
    "Select two game states, A and B: In State A the playerâ€™s sum of cards is 10, and in State B the sum of cards is 20. Plot how the value estimate of the each state changes over the number of visits to the state until the values convergence, under Monte Carlo policy evaluation and Temporal-Difference policy evaluation, respectively; so 4 plots in total.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a3e13d",
   "metadata": {},
   "source": [
    "![test3_successful](./../blackjack-main/plots/screenshot_test3_runs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63560383",
   "metadata": {},
   "source": [
    "The Monte-Carlo values over the number of visits are stable whereas the Temporal-Difference state values are more fluctuating as compared to Monte-Carlo. This is because, in Monte-Carlo, each update to the state values happens after taking the expectation over large number of simulation sequences. On the other hand, in Temporal difference, each update happens with every incoming sample.\n",
    "![MC1](../blackjack-main/plots/MC_value_vs_visit_state_A.png)\n",
    "![MC2](../blackjack-main/plots/MC_value_vs_visit_state_B.png)\n",
    "![TD1](../blackjack-main/plots/TD_value_vs_visit_state_A.png)\n",
    "![TD2](../blackjack-main/plots/TD_value_vs_visit_state_B.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d421ef97",
   "metadata": {},
   "source": [
    "### Question 9 (3 Extra Points):\n",
    "Perform Q-learning and plot how the Q-value changes over the number of visits to each action for the same two game states you selected above, until you have run Q-learning for long enough so that the Q-value converges at least on some action for each state (note: a very bad action may receive a small number of visits, so this requirement is saying you only need to wait till the better action has been visited enough times so that the Q-value of it stabilizes)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673a5dde",
   "metadata": {},
   "source": [
    "![Q1](../blackjack-main/plots/Q_value_vs_visit_state_A.png)\n",
    "![Q1](../blackjack-main/plots/Q_value_vs_visit_state_B.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974b1e42",
   "metadata": {},
   "source": [
    "Also plot the cumulative winning rate over the number of plays in the game: for every n number of plays (x-axis), show the ratio w/n (y-axis) where w is the total number of wins out of the n plays."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01b027b",
   "metadata": {},
   "source": [
    "For plotting the cumulative win-rate vs number of game plays, I have set the `self.autoQL = True` and `self.autoPlay = True`. Then, I am running sufficiently large number of iterations (=5000). In each iteration, the Q-Learning algorithm (50 simulations) runs, which updates the Q-values of the states followed by the game playing function. The cumulative winning rate stabilizes at around ~ 41 %. \n",
    "\n",
    "![cum_win_rate_vs_num_play](../blackjack-main/plots/Q_win_rate_vs_num_plays.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
